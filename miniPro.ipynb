{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvoUep/fPxMtw01Py9qsxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/college/blob/main/miniPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AngTnywOjqBp",
        "outputId": "f5b2ea93-3589-4d71-8bec-40631c7f05da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/expanded_balanced_traffic_popular_ports.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# Global variables\n",
        "scaler = None\n",
        "label_encoders = {}\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path, is_training=True):\n",
        "    \"\"\"\n",
        "    Preprocess CSV data with specific features\n",
        "    \"\"\"\n",
        "    global scaler, label_encoders\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Store original value distribution\n",
        "    if is_training:\n",
        "        categorical_distribution = {\n",
        "            col: df[col].value_counts(normalize=True) for col in ['alert', 'classification', 'protocol']\n",
        "        }\n",
        "        joblib.dump(categorical_distribution, '/content/model/categorical_distribution.pkl')\n",
        "\n",
        "    # Define numerical and categorical columns\n",
        "    numerical_columns = ['src_port', 'dst_port', 'priority']\n",
        "    categorical_columns = ['alert', 'classification', 'protocol']\n",
        "\n",
        "    # Handle categorical columns with LabelEncoder\n",
        "    for col in categorical_columns:\n",
        "        if is_training:\n",
        "            label_encoders[col] = LabelEncoder()\n",
        "            df[col] = label_encoders[col].fit_transform(df[col])\n",
        "        else:\n",
        "            df[col] = label_encoders[col].transform(df[col])\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if is_training:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "    else:\n",
        "        df[numerical_columns] = scaler.transform(df[numerical_columns])\n",
        "\n",
        "    # Store timestamp and IP addresses separately as they don't need preprocessing\n",
        "    timestamp_data = df['timestamp']\n",
        "    src_ip_data = df['src_ip']\n",
        "    dst_ip_data = df['dst_ip']\n",
        "\n",
        "    # Drop timestamp and IP columns for training\n",
        "    df = df.drop(['timestamp', 'src_ip', 'dst_ip'], axis=1)\n",
        "\n",
        "    return (df.astype(np.float32), numerical_columns, categorical_columns,\n",
        "            timestamp_data, src_ip_data, dst_ip_data)\n",
        "\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = tf.keras.layers.Input(shape=(original_dim,))\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(latent_inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        outputs = tf.keras.layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return tf.keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(data_dim, activation='tanh')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Classifier(tf.keras.Model):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    alpha = tf.random.uniform([tf.shape(real_samples)[0], 1], 0.0, 1.0)\n",
        "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        predictions = discriminator(interpolated)\n",
        "\n",
        "    gradients = tape.gradient(predictions, interpolated)\n",
        "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
        "    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n",
        "    return gradient_penalty\n",
        "\n",
        "def train_wgan(generator, discriminator, data, latent_dim, batch_size=64, epochs=100, n_critic=5, gp_weight=10.0):\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(n_critic):\n",
        "            # Train Discriminator\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                batch_indices = np.random.randint(0, data.shape[0], batch_size)\n",
        "                real_data = data[batch_indices]\n",
        "\n",
        "                z = tf.random.normal((batch_size, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                gp = gradient_penalty(discriminator, real_data, fake_data)\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + gp_weight * gp\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "        # Train Generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            z = tf.random.normal((batch_size, latent_dim))\n",
        "            fake_data = generator(z, training=True)\n",
        "            fake_output = discriminator(fake_data, training=True)\n",
        "            gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "            print(f\"Generator Loss: {gen_loss:.4f}\")\n",
        "            print(f\"Discriminator Loss: {disc_loss:.4f}\")\n",
        "\n",
        "def train_classifier(classifier, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
        "    classifier.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def predict_traffic(classifier, data, numerical_columns, categorical_columns):\n",
        "    \"\"\"\n",
        "    Predict traffic classification for new data\n",
        "    \"\"\"\n",
        "    processed_data, _, _ = preprocess_csv_with_dummies(data, is_training=False)\n",
        "    predictions = classifier.predict(processed_data)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Convert numeric predictions back to original labels\n",
        "    original_labels = label_encoders['classification'].inverse_transform(predicted_classes)\n",
        "\n",
        "    return original_labels\n",
        "\n",
        "def denormalize_data(synthetic_data, original_df, numerical_columns, categorical_columns):\n",
        "    \"\"\"\n",
        "    Denormalize synthetic data to match original format\n",
        "    \"\"\"\n",
        "    # Load categorical distribution\n",
        "    categorical_distribution = joblib.load('/content/model/categorical_distribution.pkl')\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(synthetic_data, columns=['alert', 'classification', 'priority','protocol', 'src_port', 'dst_port'])\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Round ports and priority to integers and ensure valid ranges\n",
        "        df['src_port'] = np.round(df['src_port']).astype(int).clip(1, 65535)\n",
        "        df['dst_port'] = np.round(df['dst_port']).astype(int).clip(1, 65535)\n",
        "        df['priority'] = np.round(df['priority']).astype(int).clip(1, 5)\n",
        "\n",
        "    # Convert categorical columns back to original labels\n",
        "    for col in categorical_columns:\n",
        "        # predicted_classes = np.argmax(df[col].values.reshape(-1, 1), axis=1)\n",
        "        # df[col] = label_encoders[col].inverse_transform(predicted_classes)\n",
        "        possible_values = list(categorical_distribution[col].index)\n",
        "        probabilites = list(categorical_distribution[col].values)\n",
        "\n",
        "        # Generate values based on original distribution\n",
        "        df[col] = np.random.choice(possible_values, size=len(df), p=probabilites)\n",
        "\n",
        "    # Generate timestamps in the correct format\n",
        "    base_timestamp = pd.Timestamp('2025-02-14')\n",
        "    df['timestamp'] = [\n",
        "        (base_timestamp + pd.Timedelta(seconds=i)).strftime('%m/%d-%H:%M:%S.%f')\n",
        "        for i in range(len(df))\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Generate diverse IP address\n",
        "    def generate_realistic_ip():\n",
        "        ranges = [\n",
        "            ('192.168.', 16, 31),\n",
        "            ('10.', 0, 255),\n",
        "            ('172.', 16, 31),\n",
        "            ('', 2, 255)\n",
        "        ]\n",
        "        range_choice = np.random.choice(len(ranges))\n",
        "        prefix, start, end = ranges[range_choice]\n",
        "\n",
        "        if prefix:\n",
        "            return f\"{prefix}{np.random.randint(start, end)}.\" + \\\n",
        "                   f\"{np.random.randint(0, 256)}.{np.random.randint(1, 255)}\"\n",
        "        else:\n",
        "            return f\"{np.random.randint(start, end)}.\" + \\\n",
        "                   f\"{np.random.randint(0, 256)}.\" + \\\n",
        "                   f\"{np.random.randint(0, 256)}.\" + \\\n",
        "                   f\"{np.random.randint(1, 255)}\"\n",
        "    df['src_ip'] = [generate_realistic_ip() for _ in range(len(df))]\n",
        "    df['dst_ip'] = [generate_realistic_ip() for _ in range(len(df))]\n",
        "    # Reorder columns to match original CSV\n",
        "    df = df[['timestamp', 'alert', 'classification', 'priority', 'protocol', 'src_ip', 'src_port', 'dst_ip', 'dst_port']]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def validate_synthetic_data(original_df, synthetic_df):\n",
        "    \"\"\"\n",
        "    Validate the diversity and distribution of synthetic data\n",
        "    \"\"\"\n",
        "    print(\"\\nData Distribution Analysis:\")\n",
        "\n",
        "    for col in original_df.columns:\n",
        "        print(f\"\\n{col} distribution:\")\n",
        "        print(\"\\nOriginal:\")\n",
        "        print(original_df[col].value_counts(normalize=True).head())\n",
        "        print(\"\\nSynthetic:\")\n",
        "        print(synthetic_df[col].value_counts(normalize=True).head())\n",
        "\n",
        "        if col in ['src_port', 'dst_port', 'priority']:\n",
        "            print(f\"\\n{col} statistics:\")\n",
        "            print(\"Original:\", original_df[col].describe())\n",
        "            print(\"Synthetic:\", synthetic_df[col].describe())\n",
        "\n",
        "def generate_synthetic_data(generator, num_samples, latent_dim, original_df):\n",
        "    \"\"\"\n",
        "    Generate synthetic network traffic data\n",
        "    \"\"\"\n",
        "    # Generate raw synthetic data\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generator(z, training=False).numpy()\n",
        "\n",
        "    # Denormalize the data\n",
        "    numerical_columns = ['src_port', 'dst_port', 'priority']\n",
        "    categorical_columns = ['alert', 'classification', 'protocol']\n",
        "\n",
        "    denormalized_data = denormalize_data(\n",
        "        synthetic_data,\n",
        "        original_df,\n",
        "        numerical_columns,\n",
        "        categorical_columns\n",
        "    )\n",
        "\n",
        "    return denormalized_data\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/mini.csv'\n",
        "    original_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Process data\n",
        "    (processed_data, numerical_columns, categorical_columns,\n",
        "     timestamp_data, src_ip_data, dst_ip_data) = preprocess_csv_with_dummies(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X = processed_data.values\n",
        "    y = processed_data['classification'].values\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Initialize and train models\n",
        "    original_dim = X.shape[1]\n",
        "    latent_dim = 32\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Train VAE\n",
        "    vae = VAE(original_dim, latent_dim)\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "    vae.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=50, batch_size=64)\n",
        "\n",
        "    # Train WGAN\n",
        "    generator = Generator(original_dim)\n",
        "    discriminator = Discriminator()\n",
        "    train_wgan(generator, discriminator, X_train, latent_dim)\n",
        "\n",
        "    # # Train Classifier\n",
        "    # classifier = Classifier(original_dim, num_classes)\n",
        "    # history = train_classifier(classifier, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # # Evaluate classifier\n",
        "    # test_loss, test_accuracy = classifier.evaluate(X_test, y_test)\n",
        "    # print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Save models and preprocessing objects\n",
        "    tf.keras.models.save_model(classifier, '/content/model/classifier_model.keras')\n",
        "    joblib.dump(scaler, '/content/model/scaler.pkl')\n",
        "    joblib.dump(label_encoders, '/content/model/label_encoders.pkl')\n",
        "\n",
        "    # Generate synthetic data\n",
        "    num_samples = 1000\n",
        "    synthetic_df = generate_synthetic_data(generator, num_samples, latent_dim, original_df)\n",
        "\n",
        "    # Save synthetic data\n",
        "    synthetic_df.to_csv('/content/synthetic_mini.csv', index=False)\n",
        "\n",
        "    validate_synthetic_data(original_df, synthetic_df)\n",
        "\n",
        "    # Verify data structure\n",
        "    print(\"\\nColumn names match:\",\n",
        "          all(synthetic_df.columns == original_df.columns))\n",
        "    print(\"\\nData types:\")\n",
        "    for col in synthetic_df.columns:\n",
        "        print(f\"{col}: {synthetic_df[col].dtype}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5BreHIfkYZ0",
        "outputId": "060d46f1-6f15-4a35-8cac-d2db6bd263b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 14.0358 - val_loss: 13.9603\n",
            "Epoch 2/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.7588 - val_loss: 13.9630\n",
            "Epoch 3/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 13.8254 - val_loss: 13.9609\n",
            "Epoch 4/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.7596 - val_loss: 13.9631\n",
            "Epoch 5/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 13.7859 - val_loss: 13.9582\n",
            "Epoch 6/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 13.8006 - val_loss: 13.9596\n",
            "Epoch 7/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 13.7906 - val_loss: 13.9577\n",
            "Epoch 8/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 13.7048 - val_loss: 13.9564\n",
            "Epoch 9/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.6429 - val_loss: 13.9585\n",
            "Epoch 10/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 13.8087 - val_loss: 13.9565\n",
            "Epoch 11/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.8490 - val_loss: 13.9561\n",
            "Epoch 12/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.8095 - val_loss: 13.9559\n",
            "Epoch 13/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.7897 - val_loss: 13.9567\n",
            "Epoch 14/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.7033 - val_loss: 13.9558\n",
            "Epoch 15/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.6229 - val_loss: 13.9561\n",
            "Epoch 16/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 13.6444 - val_loss: 13.9556\n",
            "Epoch 17/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.8294 - val_loss: 13.9559\n",
            "Epoch 18/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.9148 - val_loss: 13.9556\n",
            "Epoch 19/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 13.7246 - val_loss: 13.9558\n",
            "Epoch 20/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 13.8544 - val_loss: 13.9556\n",
            "Epoch 21/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 13.6081 - val_loss: 13.9553\n",
            "Epoch 22/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 13.7327 - val_loss: 13.9554\n",
            "Epoch 23/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.9154 - val_loss: 13.9562\n",
            "Epoch 24/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.7132 - val_loss: 13.9558\n",
            "Epoch 25/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.8379 - val_loss: 13.9561\n",
            "Epoch 26/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.7841 - val_loss: 13.9562\n",
            "Epoch 27/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 13.5787 - val_loss: 13.9560\n",
            "Epoch 28/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.7319 - val_loss: 13.9555\n",
            "Epoch 29/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.8450 - val_loss: 13.9562\n",
            "Epoch 30/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.8078 - val_loss: 13.9554\n",
            "Epoch 31/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.7384 - val_loss: 13.9554\n",
            "Epoch 32/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.6372 - val_loss: 13.9553\n",
            "Epoch 33/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.6853 - val_loss: 13.9557\n",
            "Epoch 34/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 13.8964 - val_loss: 13.9556\n",
            "Epoch 35/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 13.6818 - val_loss: 13.9555\n",
            "Epoch 36/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.6334 - val_loss: 13.9555\n",
            "Epoch 37/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 13.7752 - val_loss: 13.9558\n",
            "Epoch 38/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 13.7553 - val_loss: 13.9556\n",
            "Epoch 39/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 13.8057 - val_loss: 13.9563\n",
            "Epoch 40/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.7359 - val_loss: 13.9554\n",
            "Epoch 41/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 13.9251 - val_loss: 13.9554\n",
            "Epoch 42/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 13.8021 - val_loss: 13.9555\n",
            "Epoch 43/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 13.6818 - val_loss: 13.9556\n",
            "Epoch 44/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 13.8462 - val_loss: 13.9557\n",
            "Epoch 45/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.6645 - val_loss: 13.9556\n",
            "Epoch 46/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 13.9095 - val_loss: 13.9554\n",
            "Epoch 47/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 13.7988 - val_loss: 13.9556\n",
            "Epoch 48/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 13.7733 - val_loss: 13.9554\n",
            "Epoch 49/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 13.7531 - val_loss: 13.9555\n",
            "Epoch 50/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 13.8733 - val_loss: 13.9555\n",
            "Epoch 10/100\n",
            "Generator Loss: 1.2165\n",
            "Discriminator Loss: 22.2766\n",
            "Epoch 20/100\n",
            "Generator Loss: 1.6712\n",
            "Discriminator Loss: 6.7556\n",
            "Epoch 30/100\n",
            "Generator Loss: 2.0678\n",
            "Discriminator Loss: 2.4991\n",
            "Epoch 40/100\n",
            "Generator Loss: 2.6006\n",
            "Discriminator Loss: -0.0703\n",
            "Epoch 50/100\n",
            "Generator Loss: 2.7804\n",
            "Discriminator Loss: 1.1804\n",
            "Epoch 60/100\n",
            "Generator Loss: 1.9671\n",
            "Discriminator Loss: 0.4641\n",
            "Epoch 70/100\n",
            "Generator Loss: 2.1432\n",
            "Discriminator Loss: -0.1477\n",
            "Epoch 80/100\n",
            "Generator Loss: 3.2411\n",
            "Discriminator Loss: -1.6527\n",
            "Epoch 90/100\n",
            "Generator Loss: 4.1178\n",
            "Discriminator Loss: -1.7212\n",
            "Epoch 100/100\n",
            "Generator Loss: 4.8230\n",
            "Discriminator Loss: -1.7053\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9831 - loss: 0.0590 - val_accuracy: 1.0000 - val_loss: 4.7448e-07\n",
            "Epoch 2/50\n",
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.8709e-07 - val_accuracy: 1.0000 - val_loss: 7.1446e-08\n",
            "Epoch 3/50\n",
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.3176e-08 - val_accuracy: 1.0000 - val_loss: 2.2475e-08\n",
            "Epoch 4/50\n",
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.8066e-08 - val_accuracy: 1.0000 - val_loss: 9.9818e-09\n",
            "Epoch 5/50\n",
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.1602e-09 - val_accuracy: 1.0000 - val_loss: 4.7048e-09\n",
            "Epoch 6/50\n",
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.6252e-09 - val_accuracy: 1.0000 - val_loss: 2.3842e-09\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.1685e-07\n",
            "Test accuracy: 1.0000\n",
            "\n",
            "Data Distribution Analysis:\n",
            "\n",
            "timestamp distribution:\n",
            "\n",
            "Original:\n",
            "timestamp\n",
            "02/14-00:12:16.000000    0.00052\n",
            "02/14-01:00:42.000000    0.00052\n",
            "02/14-00:46:41.000000    0.00052\n",
            "02/14-00:35:11.000000    0.00052\n",
            "02/14-00:47:55.000000    0.00050\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "timestamp\n",
            "02/14-00:16:39.000000    0.001\n",
            "02/14-00:00:00.000000    0.001\n",
            "02/14-00:00:01.000000    0.001\n",
            "02/14-00:00:02.000000    0.001\n",
            "02/14-00:00:03.000000    0.001\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "alert distribution:\n",
            "\n",
            "Original:\n",
            "alert\n",
            "ICMP PING                   0.26438\n",
            "SCAN nmap XMAS              0.18466\n",
            "ICMP PING undefined code    0.17484\n",
            "ICMP PING *NIX              0.08454\n",
            "SNMP AgentX/tcp request     0.07416\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "alert\n",
            "ICMP PING                   0.285\n",
            "ICMP PING undefined code    0.193\n",
            "SCAN nmap XMAS              0.157\n",
            "ICMP PING *NIX              0.093\n",
            "SNMP trap tcp               0.074\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "classification distribution:\n",
            "\n",
            "Original:\n",
            "classification\n",
            "Misc activity    0.50052\n",
            "Normal           0.49948\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "classification\n",
            "Normal           0.507\n",
            "Misc activity    0.493\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "priority distribution:\n",
            "\n",
            "Original:\n",
            "priority\n",
            "3    0.6035\n",
            "2    0.3965\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "priority\n",
            "3    0.992\n",
            "2    0.008\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "priority statistics:\n",
            "Original: count    50000.000000\n",
            "mean         2.603500\n",
            "std          0.489175\n",
            "min          2.000000\n",
            "25%          2.000000\n",
            "50%          3.000000\n",
            "75%          3.000000\n",
            "max          3.000000\n",
            "Name: priority, dtype: float64\n",
            "Synthetic: count    1000.000000\n",
            "mean        2.992000\n",
            "std         0.089129\n",
            "min         2.000000\n",
            "25%         3.000000\n",
            "50%         3.000000\n",
            "75%         3.000000\n",
            "max         3.000000\n",
            "Name: priority, dtype: float64\n",
            "\n",
            "protocol distribution:\n",
            "\n",
            "Original:\n",
            "protocol\n",
            "TCP     0.60394\n",
            "UDP     0.29772\n",
            "ICMP    0.09834\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "protocol\n",
            "TCP     0.600\n",
            "UDP     0.293\n",
            "ICMP    0.107\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "src_ip distribution:\n",
            "\n",
            "Original:\n",
            "src_ip\n",
            "179.25.51.252     0.00052\n",
            "253.179.97.133    0.00052\n",
            "169.248.41.217    0.00052\n",
            "174.64.84.160     0.00052\n",
            "111.186.67.206    0.00050\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "src_ip\n",
            "192.168.21.245.69    0.001\n",
            "10.124.146.235       0.001\n",
            "181.66.33.46         0.001\n",
            "136.80.77.247        0.001\n",
            "172.23.223.112       0.001\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "src_port distribution:\n",
            "\n",
            "Original:\n",
            "src_port\n",
            "8443    0.05802\n",
            "1433    0.05680\n",
            "23      0.05668\n",
            "20      0.05644\n",
            "22      0.05628\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "src_port\n",
            "1       0.401\n",
            "403     0.003\n",
            "946     0.003\n",
            "1488    0.002\n",
            "4331    0.002\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "src_port statistics:\n",
            "Original: count    50000.000000\n",
            "mean      1569.568300\n",
            "std       2598.470696\n",
            "min         20.000000\n",
            "25%         25.000000\n",
            "50%        143.000000\n",
            "75%       1433.000000\n",
            "max       8443.000000\n",
            "Name: src_port, dtype: float64\n",
            "Synthetic: count    1000.000000\n",
            "mean     1349.914000\n",
            "std      1634.502147\n",
            "min         1.000000\n",
            "25%         1.000000\n",
            "50%       672.500000\n",
            "75%      2332.750000\n",
            "max      7211.000000\n",
            "Name: src_port, dtype: float64\n",
            "\n",
            "dst_ip distribution:\n",
            "\n",
            "Original:\n",
            "dst_ip\n",
            "109.172.195.216    0.00052\n",
            "76.1.195.102       0.00052\n",
            "108.182.122.97     0.00052\n",
            "186.44.84.175      0.00052\n",
            "218.153.195.30     0.00050\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "dst_ip\n",
            "10.128.23.60      0.001\n",
            "172.20.43.219     0.001\n",
            "201.153.6.236     0.001\n",
            "10.140.178.231    0.001\n",
            "172.19.242.78     0.001\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "dst_port distribution:\n",
            "\n",
            "Original:\n",
            "dst_port\n",
            "21      0.05708\n",
            "1433    0.05660\n",
            "465     0.05660\n",
            "22      0.05644\n",
            "993     0.05644\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Synthetic:\n",
            "dst_port\n",
            "1       0.350\n",
            "2722    0.002\n",
            "1511    0.002\n",
            "1159    0.002\n",
            "3221    0.002\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "dst_port statistics:\n",
            "Original: count    50000.000000\n",
            "mean      1563.234280\n",
            "std       2583.595827\n",
            "min         20.000000\n",
            "25%         25.000000\n",
            "50%        443.000000\n",
            "75%       1433.000000\n",
            "max       8443.000000\n",
            "Name: dst_port, dtype: float64\n",
            "Synthetic: count    1000.000000\n",
            "mean     1728.146000\n",
            "std      1863.534574\n",
            "min         1.000000\n",
            "25%         1.000000\n",
            "50%      1254.000000\n",
            "75%      3078.500000\n",
            "max      7634.000000\n",
            "Name: dst_port, dtype: float64\n",
            "\n",
            "Column names match: True\n",
            "\n",
            "Data types:\n",
            "timestamp: object\n",
            "alert: object\n",
            "classification: object\n",
            "priority: int64\n",
            "protocol: object\n",
            "src_ip: object\n",
            "src_port: int64\n",
            "dst_ip: object\n",
            "dst_port: int64\n"
          ]
        }
      ]
    }
  ]
}